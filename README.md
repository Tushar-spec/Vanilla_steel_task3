# Google Cloud Dataflow Pipeline for Matching Suppliers with Buyer Preferences

## ğŸ“Œ Overview
This project automates the process of matching **supplier materials** with **buyer preferences** using **Google Cloud Dataflow** and **BigQuery**.

ğŸ‘‰ Reads supplier and buyer data from **Google Cloud Storage (GCS)**  
ğŸ‘‰ Matches supplier materials to buyer preferences  
ğŸ‘‰ Stores the recommendations in **BigQuery**  

---

## ğŸš€ Prerequisites
### **1ï¸âƒ£ Set Up Google Cloud Environment**
Ensure you have:
- **Google Cloud SDK Installed**: [Install Here](https://cloud.google.com/sdk/docs/install)
- **GCP Project:** `vanilla-steel-task-2`
- **Billing Enabled** for Dataflow & BigQuery

### **2ï¸âƒ£ Enable Required GCP Services**
```sh
gcloud services enable dataflow.googleapis.com bigquery.googleapis.com storage.googleapis.com
```

---

## ğŸ“‚ File Structure
```
/gcp_dataflow_project
ğŸ‘‰ ingestion.py         # Apache Beam Dataflow Pipeline
ğŸ‘‰ buyer_preferences.csv # Buyer dataset
ğŸ‘‰ supplier_data1.csv    # Supplier dataset 1
ğŸ‘‰ supplier_data2.csv    # Supplier dataset 2
ğŸ‘‰ README.md             # Project documentation
```

---

## ğŸ”„ Step 1: Upload Files to Google Cloud Storage (GCS)
### **1ï¸âƒ£ Verify If Files Exist**
```sh
gsutil ls gs://vanila_steel_task_2/resources/task_3/
```
ğŸ‘‰ You should see:
```
gs://vanila_steel_task_2/resources/task_3/buyer_preferences.csv
gs://vanila_steel_task_2/resources/task_3/supplier_data1.csv
gs://vanila_steel_task_2/resources/task_3/supplier_data2.csv
```

### **2ï¸âƒ£ Upload Files (If Not Already Uploaded)**
```sh
gsutil cp buyer_preferences.csv gs://vanila_steel_task_2/resources/task_3/
gsutil cp supplier_data1.csv gs://vanila_steel_task_2/resources/task_3/
gsutil cp supplier_data2.csv gs://vanila_steel_task_2/resources/task_3/
```

---

## ğŸ›  Step 2: Setup BigQuery Dataset & Table
### **1ï¸âƒ£ Create a BigQuery Dataset**
```sh
bq mk --location=US vanila_steel_dataset_1
```

### **2ï¸âƒ£ Create a BigQuery Table**
```sh
bq query --use_legacy_sql=false \
'CREATE TABLE vanila_steel_dataset_1.recommendations (
   buyer_id STRING,
   supplier_id STRING,
   material_type STRING,
   price FLOAT64,
   availability STRING
);'
```

---

## ğŸ’Ÿ Step 3: Download & Run `ingestion.py` in Cloud Shell
### **1ï¸âƒ£ Download the Script from GCS**
```sh
gsutil cp gs://vanila_steel_task_2/resources/task_3/ingestion.py ~/
```

### **2ï¸âƒ£ Verify the File Exists**
```sh
ls -l ~ | grep ingestion.py
```
ğŸ‘‰ You should see:
```
-rw-r--r--  1 user cloud-users  12345 Mar 01 14:00 ingestion.py
```

---

## âš™ Step 4: Install Required Python Libraries
```sh
pip3 install apache-beam pandas google-cloud-storage google-cloud-bigquery
```

---

## ğŸ”‘ Step 5: Authenticate & Set Up GCP
```sh
gcloud auth application-default login
gcloud config set project vanilla-steel-task-2
```

---

## ğŸš€ Step 6: Run `ingestion.py` (Dataflow Pipeline)
```sh
python3 ingestion.py
```

---

## ğŸ“Š Step 7: Verify Data in BigQuery
```sh
bq query --use_legacy_sql=false \
'SELECT * FROM vanila_steel_dataset_1.recommendations LIMIT 10;'
```

---

## ğŸ“± Step 8: Monitor & Debug (If Needed)
```sh
gcloud dataflow jobs list  # Check running jobs
gcloud logging read "resource.type=dataflow_step" --limit 50  # Check logs
bq show --format=prettyjson vanila_steel_dataset_1.recommendations  # Verify schema
```

---

## ğŸ›¢ Step 9: Cleanup (Optional)
```sh
gsutil rm -r gs://vanila_steel_task_2/resources/task_3/temp/
gcloud dataflow jobs list  # Find the Job ID
gcloud dataflow jobs cancel JOB_ID
```

---

## âœ… Final Summary
âœ” **Uploaded CSV files to `gs://vanila_steel_task_2/resources/task_3/`**  
âœ” **Created BigQuery dataset (`vanila_steel_dataset_1`) and table (`recommendations`)**  
âœ” **Downloaded `ingestion.py` and installed dependencies**  
âœ” **Authenticated Google Cloud & set the correct project**  
âœ” **Ran the Python script (`ingestion.py`)**  
âœ” **Verified data in BigQuery (`vanila_steel_dataset_1.recommendations`)**  
âœ” **Monitored logs & debugged errors**  
âœ” **Cleaned up temporary files (if needed)**  

ğŸš€ **Now your Dataflow pipeline is fully automated on GCP!** ğŸ‰  
Let me know if you need help! ğŸ˜Š

